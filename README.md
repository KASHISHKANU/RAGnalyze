ğŸ§  RAGnalyze

Research-Grade RAG Evaluation, Grounding & Benchmarking Platform
RAGnalyze is a research-oriented Retrieval-Augmented Generation (RAG) benchmarking system designed to quantitatively evaluate LLM outputs beyond simple prompt â†’ response workflows.
Unlike typical RAG demos, RAGnalyze focuses on measurement, grounding, and comparison, enabling controlled experiments across models using the same retrieved evidence.

ğŸš€ Why RAGnalyze Exists

Most GenAI projects today:
Call the same OpenAI API
Use similar prompts
Judge output quality subjectively (â€œlooks goodâ€)

- RAGnalyze is different.
It answers:
Which model reasons better given the same evidence?
How grounded is each sentence in retrieved context?
How much hallucination risk exists?
How concise and efficient is the modelâ€™s reasoning?
This makes RAGnalyze a benchmarking and evaluation system, not just a chatbot.

ğŸ§© Core Features
ğŸ” Hybrid Retrieval (Production-Grade)

BM25 (keyword) + Dense Embeddings

Manual hybrid retrieval (framework-agnostic, stable)
Deduplication & controlled context assembly

ğŸ§  Controlled RAG Pipeline

Same context + same prompt across models
Eliminates evaluation bias
Enables fair model comparison

ğŸ“Š Quantitative Evaluation (Rare in Student Projects)

Semantic Faithfulness Score (embedding-based)
Compression Ratio (answer efficiency)
Latency Tracking
ROUGE-L for model-to-model answer similarity

ğŸ§ª Model Benchmarking

Side-by-side evaluation (e.g. GPT-4o vs GPT-3.5)
Delta metrics (quality vs speed trade-offs)
Winner badges (Best Reasoning, Fastest Model)

ğŸ”¬ Sentence-Level Grounding (Advanced)

Semantic similarity between each answer sentence and retrieved chunks
Sentence-level citations
Hallucination risk % per sentence
Transparent evidence inspection

ğŸ› Research-Style UI

Tabbed evaluation dashboard
Metrics, grounding, context, and comparison separated cleanly

Designed like an internal research tool, not a demo app

ğŸ§  System Architecture

User Query
   â†“
Hybrid Retrieval (BM25 + Embeddings)
   â†“
Context Assembly
   â†“
LLM Generation (Per Model)
   â†“
Evaluation Pipeline
   â”œâ”€â”€ Faithfulness
   â”œâ”€â”€ Compression
   â”œâ”€â”€ ROUGE-L
   â”œâ”€â”€ Hallucination %
   â†“
Interactive Research Dashboard


ğŸ“ Project Structure
RAGnalyze/
â”‚
â”œâ”€â”€ app.py                     # Streamlit research dashboard
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ styles.css             # Custom UI styling
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ ingestion.py           # YouTube + Web ingestion (fault-tolerant)
â”‚   â”œâ”€â”€ chunking.py            # Recursive text splitting
â”‚   â”œâ”€â”€ vector_store.py        # FAISS vector index
â”‚   â”œâ”€â”€ retriever.py           # Manual hybrid retriever
â”‚   â”œâ”€â”€ prompt_engine.py       # Strict system prompts
â”‚   â””â”€â”€ rag_pipeline.py        # Framework-agnostic RAG pipeline
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluator.py           # Unified evaluation logic
â”‚   â”œâ”€â”€ answer_metrics.py      # ROUGE + compression
â”‚   â”œâ”€â”€ grounding_metrics.py   # Semantic faithfulness
â”‚   â”œâ”€â”€ citations.py           # Sentence-level grounding
â”‚   â””â”€â”€ hallucination.py       # Hallucination % estimation
â”‚
â”œâ”€â”€ comparison/
â”‚   â””â”€â”€ model_comparator.py    # Controlled model comparisons
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“Š Metrics Explained (Interview-Ready)
Metric	What it Measures	Why it Matters
Faithfulness	Semantic alignment between answer & context	Hallucination control
Compression Ratio	Answer length vs context length	Reasoning efficiency
ROUGE-L	Similarity vs reference model	Answer quality
Latency	End-to-end response time	Production readiness
Hallucination %	Unsupported sentences	Trustworthiness

ğŸ§ª Example Evaluation Output
Model: GPT-4o
Faithfulness: 0.81
Compression Ratio: 0.19
Hallucination Risk: 9%
Latency: 6.2s

Model: GPT-3.5
Faithfulness: 0.63
Compression Ratio: 0.27
Hallucination Risk: 28%
Latency: 2.1s

ğŸ§  Key Design Decisions

Manual Hybrid Retrieval instead of fragile ensemble abstractions
Callable retrievers for framework independence
Embedding-based grounding instead of string matching
Strict data contracts using LangChain Document objects
Version-safe LangChain usage (invoke() over deprecated APIs)

ğŸ§‘â€ğŸ’» Tech Stack
Python 3.10
Streamlit (Research UI)
LangChain (Core + Community)
OpenAI (LLMs & Embeddings)
FAISS (Vector Store)
ROUGE-Score
YouTube Transcript API
yt-dlp

ğŸš€ Running Locally
conda create -n ragnalyze python=3.10
conda activate ragnalyze
pip install -r requirements.txt
streamlit run app.py

Set environment variable:
OPENAI_API_KEY=your_key_here

â˜ï¸ Deployment
Designed for Render:
No GPU required
No conda-only dependencies
Stable pip-based environment
Fault-tolerant ingestion

ğŸ§  How to Describe This in Interviews (Use This)

â€œI built a research-grade RAG benchmarking system that evaluates grounding, hallucination risk, compression efficiency, and answer quality using sentence-level semantic citations under controlled retrieval settings.â€
This is not a chatbot.
This is an evaluation system.

ğŸ”® Future Roadmap
Token & cost dashboard
One-click PDF evaluation reports
Cross-encoder re-ranking
Prompt & metric tracking (MLflow)
SaaS benchmark platform for enterprise RAG systems

â­ Why This Project Stands Out

Most people build RAG demos.
RAGnalyze builds RAG evaluation infrastructure.
That difference matters.